<html>
    <head>

        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <link href="../styles/main.css" rel="stylesheet" type="text/css">
        <link rel="shortcut icon" href="http://www.yinwang.org/images/Yc.jpg">

        <title>Propositions as programs</title>

        
        <script type="text/javascript" src="../js/title.js"></script>
    </head>

    <body>
        <div style="margin: 2% 5% 2% 5%">
            <table>
                <tr>
                    <td width="60%">
                        <div style="padding: 2% 8% 5% 8%; border: 1px solid LightGrey;">
                            <h2 onclick="nightOn()">Propositions as programs</h2>

<p>The <a href="http://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence">Curry-Howard correspondence</a> says that propositions are types, and proofs are programs. I had been wondering if there is a simpler way to think about it, so I came up with this:</p>

<blockquote><p>Propositions are programs; proofs are "abstract interpretation" of them to <code>True</code>.</p>
</blockquote>

<p>Here <code>True</code> is a meta-level truth value. A theorem prover is then an "abstract interpreter" which tries to <em>evaluate</em> the proposition to <code>True</code>. This computational process which derives <code>True</code> from the proposition, is called a <em>judgment</em>. Here I may have conflated the concept "abstract interpretation". I chose to use this term just because those two words "abstract" and "interpretation" conveys the general idea I hope to capture, but the term "abstract interpretation" here may not be what you use to think it is. I could have used "supercompilation", but I don't think the word conveys the concept very well.</p>

<p>Any way, this is a special kind of evaluation. It may be partial evaluation, supercompilation, or some sort of static analysis, whatever you call it. It can also take human inputs interactively, as long as it is semantic-preserving --- it returns <code>True</code> whenever an actual interpreter would return <code>True</code>. But it is a lot more efficient, because unlike a normal interpreter, it takes <em>shortcuts</em> (e.g. induction hypotheses). If it thus evaluates to <code>True</code>, then the proposition is proved. This is much like fortune-telling. It immediately tells you the result that an actual interpreter would eventually give you. It has a limited number of basic "reduction operations" such as unfolding, induction and rewriting, so we can record reduction sequences as "proofs", and we can verify them later on.</p>

<p>This seems to match what we have been doing with proof assistants like Coq, and possibly a more natural way of thinking about this kind of theorem proving. I feel that the propositions in Coq are a little far-fetched to be called "types" of the proof terms (note: not to be confused with tactics). For example, we can have a proof term like</p>

<pre>fun (n' : nat) (IHn' : n' + 0 = n') => ...</pre>

<p>Is <code>n' + 0 = n'</code> the type of <code>IHn'</code>? Well, you can call it whatever you like, but calling it a "type" doesn't feel natural at all. What we want is just a way to <em>bind</em> an equation to a name, so that we can refer to it. It feels better if we just think of propositions as programs with boolean return types and the proof terms reduction sequences of them into <code>True</code>. If you take a look at the proof terms of Coq, you may find that this is the case. For example, take a look at this simple theorem and the tactics that prove it:</p>

<pre>Theorem plus_0_r : forall n:nat, n + 0 = n.
Proof.
  intros n. induction n as [| n'].
  reflexivity.
  simpl. rewrite -> IHn'. reflexivity.  Qed.</pre>

<p>They produce the following proof term:</p>

<pre>plus_0_r = 
fun n : nat =>
nat_ind (fun n0 : nat => n0 + 0 = n0) eq_refl
  (fun (n' : nat) (IHn' : n' + 0 = n') =>
   eq_ind_r (fun n0 : nat => S n0 = S n') eq_refl IHn') n
     : forall n : nat, n + 0 = n</pre>

<p>You may think of this proof term as a program with the theorem as its type, but you can also think of it as a <em>reduction</em> of the program <code> n + 0 = n</code> to <code>True</code>, where <code>n</code> is a natural number. It is saying: "Do an induction where the first branch executes an equivalence judgment, and the other branch does an unfolding, a rewriting using the induction hypothesis, and an equivalence judgment." I think the second way of thinking of it is much more natural.</p>

<p>This interpretation can also explain the operation of Boyer-Moore style theorem provers (e.g. ACL2), as this is almost exactly how they work. They just don't normally output a machine-verifiable reduction sequence.</p>

<p>You may have noticed that we have an important difference from the orginal Curry-Howard correspondence here:</p>

<blockquote><p>Proofs are no longer considered programs.</p>
</blockquote>

<p>At least proofs are not object-level programs. They are the "operation histories" in the abstract interpreter, which are at the meta-level. We can give this operation history to an independent verifier, so that it can "replay" the abstract interpretation and verify the correctness of the proof.</p>

<p>Alternatively, if you consider the verifier as an interpreter then proofs are its input programs. In this sense you can also say that <em>proofs are programs for the proof verifier</em>, and both propositions and proofs can be considered programs. But they are programs at two different semantic levels: the proofs are at a <em>higher</em> level than the propositions. This is very different from Curry-Howard.</p>

<p>Thus we have arrived at a unified way of thinking about the two very different styles of theorem proving: Curry-Howard correspondence (as in Coq), and Boyer-Moore (as in ACL2).</p>


                        </div>
                      </td>
                </tr>
            </table>
        </div>
    </body>
</html>